"""
Common data generation for curvefit case study.

This module provides standardized test datasets for benchmarking across
GenJAX and NumPyro implementations. Data is generated by sampling from
the GenJAX model to ensure consistency with model assumptions.
"""

import jax.numpy as jnp
import jax.random as jrand
from genjax.pjax import seed as genjax_seed


def polyfn(x, a, b, c):
    """
    Degree 2 polynomial function: y = a + b*x + c*x^2

    Args:
        x: Input locations
        a: Constant term
        b: Linear coefficient
        c: Quadratic coefficient

    Returns:
        Polynomial values
    """
    return a + b * x + c * x**2


def generate_test_dataset(
    key=None, n_points=10, true_freq=None, true_offset=None, noise_std=None, seed=42
):
    """
    Generate a standardized test dataset by sampling from the GenJAX model.

    This ensures the synthetic data is typical for the model's prior and likelihood.
    Always samples from the model's prior to get typical parameters.

    Args:
        key: JAX random key (if None, uses seed)
        n_points: Number of data points
        true_freq: Ignored (kept for backward compatibility)
        true_offset: Ignored (kept for backward compatibility)
        noise_std: Ignored (uses model's 0.05)
        seed: Random seed (used if key is None)

    Returns:
        Dictionary with:
            - xs: Input locations (JAX array)
            - ys: Observed values (JAX array)
            - true_params: True parameters dict
            - clean_ys: Noise-free deterministic values
            - trace: Full GenJAX trace for debugging
    """
    if key is None:
        key = jrand.key(seed)

    # Import model from core
    from .core import npoint_curve

    # Generate input locations uniformly in [0, 1]
    xs = jnp.linspace(0, 1, n_points, dtype=jnp.float32)

    # Always sample from prior - apply seed transformation
    seeded_simulate = genjax_seed(npoint_curve.simulate)
    trace = seeded_simulate(key, xs)

    # Extract results from trace
    curve, (xs_ret, ys) = trace.get_retval()
    choices = trace.get_choices()

    # Extract true parameters
    true_a = float(choices["curve"]["a"])
    true_b = float(choices["curve"]["b"])
    true_c = float(choices["curve"]["c"])

    # Generate clean values for reference
    clean_ys = polyfn(xs, true_a, true_b, true_c)

    # Package results
    result = {
        "xs": xs,
        "ys": ys,
        "true_params": {
            "a": true_a,
            "b": true_b,
            "c": true_c,
            "noise_std": 0.05,  # Model's fixed noise level
        },
        "clean_ys": clean_ys,
        "trace": trace,  # Include trace for debugging
    }

    return result


def generate_easy_inference_dataset(
    seed=42, n_points=5, noise_std=0.05, param_scale=0.3, key=None  # Reduced noise to match model
):
    """
    Generate an easier dataset for importance sampling.

    This creates data that's more compatible with importance sampling from the prior:
    - Fewer data points (less concentrated posterior)
    - More observation noise (wider posterior)
    - Parameters closer to prior mean (better prior-posterior overlap)

    Args:
        seed: Random seed for reproducibility
        n_points: Number of data points (default: 5, fewer than standard)
        noise_std: Observation noise std (default: 0.2, matches updated model)
        param_scale: Scale factor for parameters (default: 0.3, keeps them near 0)
        key: Optional JAX PRNG key

    Returns:
        Dictionary with same structure as generate_test_dataset
    """
    if key is None:
        key = jrand.key(seed)

    # Generate input locations
    xs = jnp.linspace(0, 1, n_points, dtype=jnp.float32)

    # Generate parameters closer to prior mean (0)
    key, subkey = jrand.split(key)
    # These will be small values, making inference easier
    true_a = param_scale * jrand.normal(subkey, shape=())
    key, subkey = jrand.split(key)
    true_b = param_scale * jrand.normal(subkey, shape=())
    key, subkey = jrand.split(key)
    true_c = param_scale * jrand.normal(subkey, shape=())

    # Generate clean polynomial values
    clean_ys = polyfn(xs, true_a, true_b, true_c)

    # Add more noise than usual
    key, subkey = jrand.split(key)
    noise = noise_std * jrand.normal(subkey, shape=(n_points,))
    ys = clean_ys + noise

    # Package results
    result = {
        "xs": xs,
        "ys": ys,
        "true_params": {
            "a": float(true_a),
            "b": float(true_b),
            "c": float(true_c),
            "noise_std": noise_std,
        },
        "clean_ys": clean_ys,
        "n_points": n_points,
    }

    return result


def generate_fixed_dataset(
    n_points=10, x_min=0.0, x_max=1.0,
    true_a=-0.211, true_b=-0.395, true_c=0.673,
    noise_std=0.05, seed=42  # Reduced noise to match model
):
    """
    Generate a fixed dataset with specified parameters for consistent visualization.
    
    Args:
        n_points: Number of data points
        x_min, x_max: Range for x values
        true_a, true_b, true_c: True polynomial coefficients
        noise_std: Standard deviation of observation noise
        seed: Random seed for noise generation
        
    Returns:
        Dictionary with same structure as generate_test_dataset
    """
    key = jrand.key(seed)
    
    # Generate input locations
    xs = jnp.linspace(x_min, x_max, n_points, dtype=jnp.float32)
    
    # Generate clean polynomial values
    clean_ys = polyfn(xs, true_a, true_b, true_c)
    
    # Add noise
    key, subkey = jrand.split(key)
    noise = noise_std * jrand.normal(subkey, shape=(n_points,))
    ys = clean_ys + noise
    
    # Package results
    result = {
        "xs": xs,
        "ys": ys,
        "true_params": {
            "a": true_a,
            "b": true_b,
            "c": true_c,
        },
        "noise_std": noise_std,
        "clean_ys": clean_ys,
    }
    
    return result


def generate_dataset_with_outliers(
    key=None,
    n_points=20,
    outlier_fraction=0.15,
    outlier_scale=3.0,
    noise_std=0.05,  # Reduced noise to match model
    seed=42,
    true_a=None,
    true_b=None,
    true_c=None,
):
    """
    Generate a dataset with outliers for testing robust regression.

    Args:
        key: JAX random key (if None, uses seed)
        n_points: Number of data points
        outlier_fraction: Fraction of points to make outliers (default 0.15)
        outlier_scale: How many standard deviations away outliers can be (default 3.0)
        noise_std: Base noise level for inliers (default 0.2)
        seed: Random seed (used if key is None)
        true_a, true_b, true_c: True polynomial coefficients (sampled if None)

    Returns:
        Dictionary with:
            - xs: Input locations
            - ys: Observed values (with outliers)
            - true_params: True parameters including outlier info
            - clean_ys: Noise-free deterministic values
            - is_outlier: Boolean array indicating which points are outliers
    """
    if key is None:
        key = jrand.key(seed)

    # Generate input locations
    xs = jnp.linspace(0, 1, n_points, dtype=jnp.float32)

    # Generate or use provided coefficients
    if true_a is None:
        key, subkey = jrand.split(key)
        true_a = jrand.normal(subkey, shape=()) * 0.5
    if true_b is None:
        key, subkey = jrand.split(key)
        true_b = jrand.normal(subkey, shape=()) * 0.5
    if true_c is None:
        key, subkey = jrand.split(key)
        true_c = jrand.normal(subkey, shape=()) * 0.5

    # Generate clean polynomial values
    clean_ys = polyfn(xs, true_a, true_b, true_c)

    # Determine which points are outliers
    key, subkey = jrand.split(key)
    is_outlier = jrand.uniform(subkey, shape=(n_points,)) < outlier_fraction

    # Generate noise
    key, subkey = jrand.split(key)
    inlier_noise = noise_std * jrand.normal(subkey, shape=(n_points,))

    key, subkey = jrand.split(key)
    outlier_noise = noise_std * outlier_scale * jrand.normal(subkey, shape=(n_points,))

    # Combine inlier and outlier noise
    noise = jnp.where(is_outlier, outlier_noise, inlier_noise)
    ys = clean_ys + noise

    # Package results
    result = {
        "xs": xs,
        "ys": ys,
        "true_params": {
            "a": float(true_a),
            "b": float(true_b),
            "c": float(true_c),
            "noise_std": noise_std,
            "outlier_fraction": outlier_fraction,
            "outlier_scale": outlier_scale,
        },
        "clean_ys": clean_ys,
        "is_outlier": is_outlier,
        "n_outliers": int(jnp.sum(is_outlier)),
    }

    return result


def print_dataset_summary(data_dict, name="Dataset"):
    """
    Print a summary of the dataset characteristics.

    Args:
        data_dict: Dictionary from generate_test_dataset
        name: Name for the dataset
    """
    print(f"\n=== {name} Summary ===")
    print(f"  Number of points: {len(data_dict['xs'])}")
    print("  True polynomial coefficients:")
    print(f"    a (const): {data_dict['true_params']['a']:.3f}")
    print(f"    b (x):     {data_dict['true_params']['b']:.3f}")
    print(f"    c (xÂ²):    {data_dict['true_params']['c']:.3f}")
    print(f"  Noise std: {data_dict['true_params']['noise_std']:.3f}")

    if "is_outlier" in data_dict:
        print(
            f"  Number of outliers: {data_dict['n_outliers']} ({data_dict['true_params']['outlier_fraction'] * 100:.0f}%)"
        )
        print(
            f"  Outlier scale: {data_dict['true_params']['outlier_scale']:.1f}x noise"
        )

    print(f"  Y range: [{data_dict['ys'].min():.3f}, {data_dict['ys'].max():.3f}]")
